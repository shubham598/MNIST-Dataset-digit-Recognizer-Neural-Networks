
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the following details here\n",
    "** Name: ** `SHUBHAM KUMAR `<br/>\n",
    "** Roll Number: ** `16CE30018`<br/>\n",
    "** Department: ** `CIVIL ENGINEERING`<br/>\n",
    "** Email: ** `SHUBHAMKUMAR598@GMAIL.COM `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "To run and solve this assignment, one must have a working IPython Notebook installation. The easiest way to set it up for both Windows and Linux is to install [Anaconda](https://www.continuum.io/downloads). Then save this file ([`assignment_01.ipynb`]()) to your computer, run Anaconda and choose this file in Anaconda's file explorer. Use `Python 3` version. Below statements assume that you have already followed these instructions. If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: You will implement a fully connected neural network from scratch in this problem\n",
    "We marked places where you are expected to add/change your own code with **`##### write your code below #####`** comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "597wDiAvGvuB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are not supposed to import any other python library to work on this assignments.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "'''You are not supposed to import any other python library to work on this assignments.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B54oZmm1DNWe",
    "outputId": "8c59bd48-230d-4fb9-eba1-82471de363df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''data is loaded from data directory.\n",
    "please don't remove the folder '''\n",
    "\n",
    "x_train = np.load('./data/X_train.npy')\n",
    "x_train = x_train.flatten().reshape(-1,28*28)\n",
    "x_train = x_train / 255.0\n",
    "gt_indices = np.load('./data/y_train.npy')\n",
    "train_length = len(x_train)\n",
    "print(\"Number of training examples: {:d}\".format(train_length))\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LvVFhXNB5xrD"
   },
   "outputs": [],
   "source": [
    "'''Dimensions to be used for creating your model'''\n",
    "\n",
    "batch_size = 64  # batch size\n",
    "input_dim = 784  # input dimension\n",
    "hidden_1_dim = 512  # hidden layer 1 dimension\n",
    "hidden_2_dim = 256  # hidden layer 2 dimension\n",
    "output_dim = 10   # output dimension\n",
    "\n",
    "'''Other hyperparameters'''\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hImaaujc5zXg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "#creating one hot vector representation of output classification\n",
    "y_train = np.zeros((train_length, output_dim))\n",
    " #print(y.shape, gt_indices.shape)\n",
    "for i in range(train_length):\n",
    "    y_train[i,gt_indices[i]] = 1\n",
    "\n",
    "# Number of mini-batches (as integer) in one epoch\n",
    "num_minibatches = np.floor(train_length/batch_size).astype(int) \n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W7lHWEWVaVlK",
    "outputId": "4ecb1bfc-4568-44cb-e109-57677da50eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of mini-batches 937 and total training data used in training:59968.\n"
     ]
    }
   ],
   "source": [
    "print(\"No of mini-batches {:d} and total training data used in training:\\\n",
    "{}.\".format(num_minibatches, num_minibatches*batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9HRf0Wj52cK"
   },
   "outputs": [],
   "source": [
    "'''Randomly Initialize Weights  from standard normal distribution (i.e., mean = 0 and s.d. = 1.0).\n",
    "Use the dimesnions specified in the cell 3 to initialize your weights matrices. \n",
    "Use the nomenclature W1,W2 etc. (provided below) for the different weight matrices.'''\n",
    "\n",
    "########################## write your code below ##############################################\n",
    "W1 = np.random.normal(loc =0.0, scale = 1.0, size =(input_dim, hidden_1_dim))\n",
    "W2 = np.random.normal(loc =0.0, scale = 1.0, size =(hidden_1_dim,hidden_2_dim))\n",
    "W3 = np.random.normal(loc =0.0, scale = 1.0, size =(hidden_2_dim,output_dim))\n",
    "\n",
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmZRrEVb6CJy"
   },
   "outputs": [],
   "source": [
    "# Write a function which computes the softmax where X is vector of scores computed during forward pass\n",
    "def softmax(x):\n",
    "    ##############################write your code here #################################\n",
    "    x = x - np.max(x, 1).reshape((-1, 1)) + 0.0000001\n",
    "    ####################################################################################\n",
    "    return np.exp(x)/np.expand_dims(np.sum(np.exp(x), 1), 1)\n",
    "####################################################################################\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "Gjz4yhwE6JQw",
    "outputId": "341578db-29a4-48ca-b0f8-a0343aadd24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 0, iteration: 0, Loss: 16.4059 \n",
      " Epoch: 1, iteration: 937, Loss: 2.5904 \n",
      " Epoch: 2, iteration: 1874, Loss: 2.3026 \n",
      " Epoch: 3, iteration: 2811, Loss: 2.3026 \n",
      " Epoch: 4, iteration: 3748, Loss: 2.0146 \n",
      " Epoch: 5, iteration: 4685, Loss: 1.7358 \n",
      " Epoch: 6, iteration: 5622, Loss: 1.7269 \n",
      " Epoch: 7, iteration: 6559, Loss: 1.4392 \n",
      " Epoch: 8, iteration: 7496, Loss: 1.2124 \n",
      " Epoch: 9, iteration: 8433, Loss: 0.8486 \n",
      " Epoch: 10, iteration: 9370, Loss: 0.5756 \n",
      " Epoch: 11, iteration: 10307, Loss: 0.5756 \n",
      " Epoch: 12, iteration: 11244, Loss: 0.5756 \n",
      " Epoch: 13, iteration: 12181, Loss: 0.5756 \n",
      " Epoch: 14, iteration: 13118, Loss: 0.5756 \n",
      " Epoch: 15, iteration: 14055, Loss: 0.5756 \n",
      " Epoch: 16, iteration: 14992, Loss: 0.5756 \n",
      " Epoch: 17, iteration: 15929, Loss: 0.5756 \n",
      " Epoch: 18, iteration: 16866, Loss: 0.5756 \n",
      " Epoch: 19, iteration: 17803, Loss: 0.5756 \n",
      " Epoch: 20, iteration: 18740, Loss: 0.5756 \n",
      " Epoch: 21, iteration: 19677, Loss: 0.5756 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "no_of_iterations = 20000\n",
    "loss_list=[]\n",
    "i_epoch = 0\n",
    "for i_iter in range(no_of_iterations):\n",
    "    \n",
    "    ''''''\n",
    "    batch_elem_idx = i_iter%num_minibatches\n",
    "    x_batchinput = x_train[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size]\n",
    "    \n",
    "    ########################## write your code below ##############################################\n",
    "    ######################### Forward Pass Block #####################################\n",
    "    '''Write the code for forward block of the neural network with 2 hidden layers.\n",
    "    Please stick to the notation below which follows the notation provided in the lecture slides.\n",
    "    Note that you are allowed to write the right hand sides of these variables in more than\n",
    "    one line if that is convenient for you.'''\n",
    "    \n",
    "    # first hidden layer implementation\n",
    "    a1 = np.matmul(x_batchinput, W1)\n",
    "    # implement Relu layer\n",
    "    h1 = np.maximum(0, a1)\n",
    "    #  implement 2 hidden layer\n",
    "    a2 = np.matmul(h1, W2)\n",
    "    # implement Relu activation \n",
    "    h2 = np.maximum(0, a2)\n",
    "    #implement linear output layer\n",
    "    a3 = np.matmul(h2, W3)\n",
    "    # softmax layer\n",
    "    softmax_score = softmax(a3) #enusre you have implemented the softmax function defined above\n",
    "    \n",
    "    ##################################################################################\n",
    "    ###############################################################################################\n",
    "    \n",
    "    neg_log_softmax_score = -np.log(softmax_score+0.00000001) # The small number is added to avoid 0 input to log function\n",
    "    \n",
    "    # Compute and print loss\n",
    "    if i_iter%num_minibatches == 0:\n",
    "        loss = np.mean(np.diag(np.take(neg_log_softmax_score, gt_indices[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size],\\\n",
    "                                       axis=1)))\n",
    "        print(\" Epoch: {:d}, iteration: {:d}, Loss: {:6.4f} \".format(i_epoch, i_iter, loss))\n",
    "        loss_list.append(loss)\n",
    "        i_epoch += 1\n",
    "        # Each 10th epoch reduce learning rate by a factor of 10\n",
    "        if i_epoch%10 == 0:\n",
    "            learning_rate /= 10.0\n",
    "     \n",
    "    ################################### Backpropagation Code Block #####################################\n",
    "    ''' Use the convention grad_{} for computing the gradients.\n",
    "    for e.g \n",
    "        grad_W1 for gradients w.r.t. weight W1\n",
    "        grad_w2 for gradients w.r.t. weights W2'''\n",
    "    \n",
    "    ########################## write your code below ##############################################\n",
    "    # Gradient of cross-entropy loss w.r.t. preactivation of the output layer\n",
    "    grad_softmax_score = softmax_score - y_train[batch_elem_idx*batch_size : (batch_elem_idx + 1)*batch_size]\n",
    "    \n",
    "    \n",
    "    # gradient w.r.t W3\n",
    "    grad_W3 = np.matmul(h2.transpose(), grad_softmax_score)\n",
    "    # gradient w.r.t h2\n",
    "    grad_h2 = np.matmul( grad_softmax_score, W3.transpose())\n",
    "    # gradient w.r.t a2\n",
    "    grad_a2 = np.zeros(a2.shape, dtype = np.float64)\n",
    "    grad_a2[a2 > 0] = grad_h2[a2 > 0]\n",
    "    # gradient w.r.t W2\n",
    "    grad_W2 = np.matmul(h1.transpose(), grad_a2)\n",
    "    # gradient w.r.t h1\n",
    "    grad_h1 = np.matmul(grad_a2, W2.transpose())\n",
    "    # gradient w.r.t a1\n",
    "    grad_a1 = np.zeros(a1.shape, dtype = np.float64)\n",
    "    grad_a1[a1 > 0] = grad_h1[a1 > 0]\n",
    "    # gradient w.r.t W1\n",
    "    grad_W1 = np.matmul(x_batchinput.transpose(), grad_a1)\n",
    "    \n",
    "    ###############################################################################################\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    ################################ Update Weights Block using SGD ####################################\n",
    "    W3 -= learning_rate * grad_W3\n",
    "    W2 -= learning_rate * grad_W2\n",
    "    W1 -= learning_rate * grad_W1\n",
    "    ####################################################################################################\n",
    "    \n",
    "#plotting the loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(loss_list)\n",
    "plt.title('Loss vs epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Loading the test data from data/X_test.npy and data/y_test.npy.'''\n",
    "x_test = np.load('./data/X_test.npy')\n",
    "x_test = x_test.flatten().reshape(-1,28*28)\n",
    "x_test = x_test / 255.0\n",
    "y_test = np.load('./data/y_test.npy')\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 91.78 %\n"
     ]
    }
   ],
   "source": [
    "batch_size_test = 100 # Deliberately taken 100 so that it divides the test data size\n",
    "num_minibatches = len(y_test)/batch_size_test\n",
    "test_correct = 0\n",
    "\n",
    "'''Only forward block code and compute softmax_score .'''\n",
    "for i_iter in range(int(num_minibatches)):\n",
    "    \n",
    "    '''Get one minibatch'''\n",
    "    batch_elem_idx = i_iter%num_minibatches\n",
    "    x_batchinput = x_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "    \n",
    "    ######### copy only the forward pass block of your code and pass the x_batchinput to it and compute softmax_score ##########\n",
    "    # first hidden layer implementation\n",
    "    # first hidden layer implementation\n",
    "    a1 = np.matmul(x_batchinput, W1)\n",
    "    # implement Relu layer\n",
    "    h1 = np.maximum(0, a1)\n",
    "    #  implement 2 hidden layer\n",
    "    a2 = np.matmul(h1, W2)\n",
    "    # implement Relu activation \n",
    "    h2 = np.maximum(0, a2)\n",
    "    #implement linear output layer\n",
    "    a3 = np.matmul(h2, W3)\n",
    "    # softmax layer\n",
    "    softmax_score = softmax(a3) #enusre you have implemented the softmax function defined above\n",
    "    ##################################################################################\n",
    "    \n",
    "    y_batchinput = y_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "    \n",
    "    y_pred = np.argmax(softmax_score, axis=1)\n",
    "    num_correct_i_iter = np.sum(y_pred == y_batchinput)\n",
    "    test_correct += num_correct_i_iter\n",
    "print (\"Test accuracy is {:4.2f} %\".format(test_correct/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_Hidden_MLP_New.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

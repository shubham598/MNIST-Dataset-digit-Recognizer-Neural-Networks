{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Name: ** `SHUBHAM KUMAR`<br/>\n",
    "** Roll Number: ** `16CE30018`<br/>\n",
    "** Department: ** `CIVIL ENGINEERING`<br/>\n",
    "** Email: ** `SHUBHAMKUMAR598@GMAIL.COM `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "To run and solve this assignment, one must have a working IPython Notebook installation. The easiest way to set it up for both Windows and Linux is to install [Anaconda](https://www.continuum.io/downloads). Then save this file to your computer, run Anaconda and choose this file in Anaconda's file explorer. Use `Python 3` version. Below statements assume that you have already followed these instructions. If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aim : Implement a fully connected neural network from scratch in this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "597wDiAvGvuB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B54oZmm1DNWe",
    "outputId": "8c59bd48-230d-4fb9-eba1-82471de363df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''data is loaded from data directory.\n",
    "please don't remove the folder '''\n",
    "\n",
    "x_train = np.load('./data/X_train.npy')\n",
    "x_train = x_train.flatten().reshape(-1,28*28)\n",
    "x_train = x_train / 255.0\n",
    "gt_indices = np.load('./data/y_train.npy')\n",
    "train_length = len(x_train)\n",
    "print(\"Number of training examples: {:d}\".format(train_length))\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LvVFhXNB5xrD"
   },
   "outputs": [],
   "source": [
    "'''Dimensions to be used for creating model'''\n",
    "\n",
    "batch_size = 64  # batch size\n",
    "input_dim = 784  # input dimension\n",
    "hidden_1_dim = 512  # hidden layer 1 dimension\n",
    "hidden_2_dim = 256  # hidden layer 2 dimension\n",
    "output_dim = 10   # output dimension\n",
    "\n",
    "'''Other hyperparameters'''\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hImaaujc5zXg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "#creating one hot vector representation of output classification\n",
    "y_train = np.zeros((train_length, output_dim))\n",
    " #print(y.shape, gt_indices.shape)\n",
    "for i in range(train_length):\n",
    "    y_train[i,gt_indices[i]] = 1\n",
    "\n",
    "# Number of mini-batches (as integer) in one epoch\n",
    "num_minibatches = np.floor(train_length/batch_size).astype(int) \n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W7lHWEWVaVlK",
    "outputId": "4ecb1bfc-4568-44cb-e109-57677da50eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of mini-batches 937 and total training data used in training:59968.\n"
     ]
    }
   ],
   "source": [
    "print(\"No of mini-batches {:d} and total training data used in training:\\\n",
    "{}.\".format(num_minibatches, num_minibatches*batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9HRf0Wj52cK"
   },
   "outputs": [],
   "source": [
    "'''Randomly Initialize Weights  from standard normal distribution (i.e., mean = 0 and s.d. = 1.0).\n",
    "Using the dimesnions specified in the cell 3 to initialize  weights matrices. \n",
    "Using the nomenclature W1,W2 etc. (provided below) for the different weight matrices.'''\n",
    "\n",
    "##########################  code ##############################################\n",
    "W1 = np.random.normal(loc =0.0, scale = 1.0, size =(input_dim, hidden_1_dim))\n",
    "W2 = np.random.normal(loc =0.0, scale = 1.0, size =(hidden_1_dim,hidden_2_dim))\n",
    "W3 = np.random.normal(loc =0.0, scale = 1.0, size =(hidden_2_dim,output_dim))\n",
    "\n",
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmZRrEVb6CJy"
   },
   "outputs": [],
   "source": [
    "# Softmax  function\n",
    "def softmax(x):\n",
    "    ##############################################################\n",
    "    x = x - np.max(x, 1).reshape((-1, 1)) + 0.0000001\n",
    "    ####################################################################################\n",
    "    return np.exp(x)/np.expand_dims(np.sum(np.exp(x), 1), 1)\n",
    "    ####################################################################################\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "Gjz4yhwE6JQw",
    "outputId": "341578db-29a4-48ca-b0f8-a0343aadd24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 0, iteration: 0, Loss: 16.9816 \n",
      " Epoch: 1, iteration: 937, Loss: 2.8755 \n",
      " Epoch: 2, iteration: 1874, Loss: 2.1043 \n",
      " Epoch: 3, iteration: 2811, Loss: 2.0148 \n",
      " Epoch: 4, iteration: 3748, Loss: 1.7269 \n",
      " Epoch: 5, iteration: 4685, Loss: 1.7273 \n",
      " Epoch: 6, iteration: 5622, Loss: 1.7269 \n",
      " Epoch: 7, iteration: 6559, Loss: 1.7269 \n",
      " Epoch: 8, iteration: 7496, Loss: 1.4327 \n",
      " Epoch: 9, iteration: 8433, Loss: 1.1513 \n",
      " Epoch: 10, iteration: 9370, Loss: 1.1698 \n",
      " Epoch: 11, iteration: 10307, Loss: 1.1613 \n",
      " Epoch: 12, iteration: 11244, Loss: 1.1443 \n",
      " Epoch: 13, iteration: 12181, Loss: 0.9642 \n",
      " Epoch: 14, iteration: 13118, Loss: 0.8639 \n",
      " Epoch: 15, iteration: 14055, Loss: 0.8635 \n",
      " Epoch: 16, iteration: 14992, Loss: 0.8635 \n",
      " Epoch: 17, iteration: 15929, Loss: 0.8635 \n",
      " Epoch: 18, iteration: 16866, Loss: 0.8635 \n",
      " Epoch: 19, iteration: 17803, Loss: 0.8635 \n",
      " Epoch: 20, iteration: 18740, Loss: 0.8635 \n",
      " Epoch: 21, iteration: 19677, Loss: 0.8635 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "no_of_iterations = 20000\n",
    "loss_list=[]\n",
    "i_epoch = 0\n",
    "for i_iter in range(no_of_iterations):\n",
    "    \n",
    "    ''''''\n",
    "    batch_elem_idx = i_iter%num_minibatches\n",
    "    x_batchinput = x_train[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size]\n",
    "    \n",
    "    ######################### Forward Pass Block #####################################\n",
    "    '''code for forward block of the neural network with 2 hidden layers.\n",
    "    '''\n",
    "    \n",
    "    # first hidden layer implementation\n",
    "    a1 = np.matmul(x_batchinput, W1)\n",
    "    # implement Relu layer\n",
    "    h1 = np.maximum(0, a1)\n",
    "    #  implement 2 hidden layer\n",
    "    a2 = np.matmul(h1, W2)\n",
    "    # implement Relu activation \n",
    "    h2 = np.maximum(0, a2)\n",
    "    #implement linear output layer\n",
    "    a3 = np.matmul(h2, W3)\n",
    "    # softmax layer\n",
    "    softmax_score = softmax(a3)\n",
    "    \n",
    "    ##################################################################################\n",
    "    ###############################################################################################\n",
    "    \n",
    "    neg_log_softmax_score = -np.log(softmax_score+0.00000001) # The small number is added to avoid 0 input to log function\n",
    "    \n",
    "    # Compute and print loss\n",
    "    y_batch = y_train[batch_elem_idx*batch_size : (batch_elem_idx + 1)*batch_size]\n",
    "    \n",
    "    if i_iter%num_minibatches == 0:\n",
    "        loss = np.mean(np.diag(np.take(neg_log_softmax_score, gt_indices[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size],\\\n",
    "                                       axis=1)))\n",
    "        print(\" Epoch: {:d}, iteration: {:d}, Loss: {:6.4f} \".format(i_epoch, i_iter, loss))\n",
    "        loss_list.append(loss)\n",
    "        i_epoch += 1\n",
    "        # Each 10th epoch reduce learning rate by a factor of 10\n",
    "        if i_epoch%10 == 0:\n",
    "            learning_rate /= 10.0\n",
    "     \n",
    "    ################################### Backpropagation Code Block #####################################\n",
    "    ''' Use the convention grad_{} for computing the gradients.\n",
    "    for e.g \n",
    "        grad_W1 for gradients w.r.t. weight W1\n",
    "        grad_w2 for gradients w.r.t. weights W2'''\n",
    "    \n",
    "    # Gradient of cross-entropy loss w.r.t. preactivation of the output layer\n",
    "    grad_softmax_score = softmax_score - y_batch\n",
    "    \n",
    "    # gradient w.r.t W3\n",
    "    grad_W3 = np.matmul(h2.transpose(), grad_softmax_score)\n",
    "    # gradient w.r.t h2\n",
    "    grad_h2 = np.matmul( grad_softmax_score, W3.transpose())\n",
    "    # gradient w.r.t a2\n",
    "    grad_a2 = np.zeros(a2.shape, dtype = np.float64)\n",
    "    grad_a2[a2 > 0] = grad_h2[a2 > 0]\n",
    "    # gradient w.r.t W2\n",
    "    grad_W2 = np.matmul(h1.transpose(), grad_a2)\n",
    "    # gradient w.r.t h1\n",
    "    grad_h1 = np.matmul(grad_a2, W2.transpose())\n",
    "    # gradient w.r.t a1\n",
    "    grad_a1 = np.zeros(a1.shape, dtype = np.float64)\n",
    "    grad_a1[a1 > 0] = grad_h1[a1 > 0]\n",
    "    # gradient w.r.t W1\n",
    "    grad_W1 = np.matmul(x_batchinput.transpose(), grad_a1)\n",
    "    \n",
    "    ###############################################################################################\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    ################################ Updating Weights Block using SGD ####################################\n",
    "    W3 -= learning_rate * grad_W3\n",
    "    W2 -= learning_rate * grad_W2\n",
    "    W1 -= learning_rate * grad_W1\n",
    "    ####################################################################################################\n",
    "    \n",
    "#plotting the loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(loss_list)\n",
    "plt.title('Loss vs epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Loading the test data from data/X_test.npy and data/y_test.npy.'''\n",
    "x_test = np.load('./data/X_test.npy')\n",
    "x_test = x_test.flatten().reshape(-1,28*28)\n",
    "x_test = x_test / 255.0\n",
    "y_test = np.load('./data/y_test.npy')\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 91.76 %\n"
     ]
    }
   ],
   "source": [
    "batch_size_test = 100 # Deliberately taken 100 so that it divides the test data size\n",
    "num_minibatches = len(y_test)/batch_size_test\n",
    "test_correct = 0\n",
    "\n",
    "'''Only forward block code and compute softmax_score .'''\n",
    "for i_iter in range(int(num_minibatches)):\n",
    "    \n",
    "    '''Get one minibatch'''\n",
    "    batch_elem_idx = i_iter%num_minibatches\n",
    "    x_batchinput = x_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "    \n",
    "    ###################\n",
    "    # first hidden layer implementation\n",
    "    # first hidden layer implementation\n",
    "    a1 = np.matmul(x_batchinput, W1)\n",
    "    # implement Relu layer\n",
    "    h1 = np.maximum(0, a1)\n",
    "    #  implement 2 hidden layer\n",
    "    a2 = np.matmul(h1, W2)\n",
    "    # implement Relu activation \n",
    "    h2 = np.maximum(0, a2)\n",
    "    #implement linear output layer\n",
    "    a3 = np.matmul(h2, W3)\n",
    "    # softmax layer\n",
    "    softmax_score = softmax(a3) \n",
    "    ##################################################################################\n",
    "    \n",
    "    y_batchinput = y_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "    \n",
    "    y_pred = np.argmax(softmax_score, axis=1)\n",
    "    num_correct_i_iter = np.sum(y_pred == y_batchinput)\n",
    "    test_correct += num_correct_i_iter\n",
    "print (\"Test accuracy is {:4.2f} %\".format(test_correct/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_Hidden_MLP_New.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
